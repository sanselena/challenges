# Import libraries
import os
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.utils import resample

# Step 1: Load the dataset
def load_airbus_dataset(data_dir, max_images=30000):  # Increased to 30k images
    # Load the CSV file with ship segmentations
    csv_path = os.path.join(data_dir, "train_ship_segmentations_v2.csv")
    
    # Read the CSV file
    df = pd.read_csv(csv_path)
    
    # Create a new column: 1 if the image has a ship, 0 otherwise
    df["has_ship"] = df["EncodedPixels"].notna().astype(int)
    
    # Remove duplicate file names (an image can have multiple segmentation masks)
    df = df.groupby("ImageId")["has_ship"].max().reset_index()
    
    # Get full image file paths
    df["ImagePath"] = df["ImageId"].apply(lambda x: os.path.join(data_dir, "train_v2", x))

    # Balance the dataset by oversampling the minority class (ships)
    df_ship = df[df["has_ship"] == 1]
    df_no_ship = df[df["has_ship"] == 0]
    
    # Oversample the minority class (ships)
    df_ship_oversampled = resample(df_ship, replace=True, n_samples=len(df_no_ship), random_state=42)
    
    # Combine the oversampled data
    df_balanced = pd.concat([df_no_ship, df_ship_oversampled])
    
    # Limit the dataset to max_images
    if len(df_balanced) > max_images:
        df_balanced = df_balanced.sample(max_images, random_state=42)  # Randomly sample max_images

    return df_balanced["ImagePath"].values, df_balanced["has_ship"].values

# Step 2: Load and resize a single image
def load_and_resize_image(img_path, target_size=(224, 224)):
    try:
        img_pil = Image.open(img_path).convert("RGB")
        img_resized = img_pil.resize(target_size)
        return np.array(img_resized, dtype=np.uint8)
    except Exception as e:
        print(f"Error loading image {img_path}: {e}")
        return None  # Return None for failed images

# Step 3: Preprocess and save data in batches
def preprocess_and_save_data(image_paths, labels, target_size=(224, 224), batch_size=5000, output_dir="data"):
    os.makedirs(output_dir, exist_ok=True)
    
    # Define augmentation
    datagen = ImageDataGenerator(
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    
    num_batches = (len(image_paths) + batch_size - 1) // batch_size
    
    for batch_idx in range(num_batches):
        start = batch_idx * batch_size
        end = min((batch_idx + 1) * batch_size, len(image_paths))
        
        batch_paths = image_paths[start:end]
        batch_labels = labels[start:end]
        
        # Process images in the current batch
        resized_images = []
        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(tqdm(executor.map(load_and_resize_image, batch_paths), total=len(batch_paths), desc=f"Processing batch {batch_idx + 1}/{num_batches}"))
        
        # Remove None values (failed loads)
        resized_images = [img for img in results if img is not None]
        batch_labels = batch_labels[:len(resized_images)]  # Adjust labels to match
        
        # Apply augmentation
        augmented_images = []
        for img in resized_images:
            augmented_images.append(datagen.random_transform(img))
        
        # Convert list to NumPy array
        augmented_images = np.array(augmented_images, dtype=np.uint8)
        
        # Normalize pixel values to [0, 1]
        augmented_images = augmented_images.astype(np.float32) / 255.0
        
        # Save the batch (both images and labels in the same file)
        np.savez_compressed(
            os.path.join(output_dir, f"batch_{batch_idx}.npz"),
            images=augmented_images,  # Save augmented images
            labels=batch_labels       # Save labels
        )
        
        print(f"Saved batch {batch_idx + 1}/{num_batches} with {len(augmented_images)} images.")

# Step 4: Split data into train/validation/test sets
def split_and_save_data(output_dir="data"):
    # Initialize lists to store splits
    X_train, X_val, X_test = [], [], []
    y_train, y_val, y_test = [], [], []
    
    # Process each batch
    batch_idx = 0
    while True:
        batch_path = os.path.join(output_dir, f"batch_{batch_idx}.npz")
        if not os.path.exists(batch_path):
            break  # Stop if no more batches
        
        # Load the batch
        with np.load(batch_path) as data:
            X_batch = data["images"]  # Use "images" key
            y_batch = data["labels"]  # Use "labels" key
        
        # Split the batch into train/validation/test sets
        X_train_batch, X_test_batch, y_train_batch, y_test_batch = train_test_split(X_batch, y_batch, test_size=0.2, random_state=42)
        X_val_batch, X_test_batch, y_val_batch, y_test_batch = train_test_split(X_test_batch, y_test_batch, test_size=0.5, random_state=42)
        
        # Save the splits incrementally
        np.savez_compressed(os.path.join(output_dir, f"X_train_batch_{batch_idx}.npz"), X_train_batch)
        np.savez_compressed(os.path.join(output_dir, f"X_val_batch_{batch_idx}.npz"), X_val_batch)
        np.savez_compressed(os.path.join(output_dir, f"X_test_batch_{batch_idx}.npz"), X_test_batch)
        np.savez_compressed(os.path.join(output_dir, f"y_train_batch_{batch_idx}.npz"), y_train_batch)
        np.savez_compressed(os.path.join(output_dir, f"y_val_batch_{batch_idx}.npz"), y_val_batch)
        np.savez_compressed(os.path.join(output_dir, f"y_test_batch_{batch_idx}.npz"), y_test_batch)
        
        print(f"Processed and saved batch {batch_idx + 1}.")
        batch_idx += 1
    
    print("Data split and saved successfully!")

# Step 5: Visualize some images
def visualize_images(images, labels, num_samples=5):
    plt.figure(figsize=(15, 5))
    
    for i in range(num_samples):
        idx = np.random.randint(0, len(images))
        image = images[idx]
        label = labels[idx]

        plt.subplot(1, num_samples, i + 1)
        plt.imshow(image)
        plt.title(f"Label: {label}\nShape: {image.shape}")
        plt.axis("off")
    
    plt.tight_layout()
    plt.show()

# Step 6: Main function
def main():
    # Path to dataset
    data_dir = r"C:\Users\Victus\ShipsNet_Project\dataset"
    
    # Load dataset (image paths & labels)
    image_paths, labels = load_airbus_dataset(data_dir, max_images=30000)  # Increased to 30k images

    print(f"Loaded {len(image_paths)} images with labels.")

    # Preprocess and save data in batches
    preprocess_and_save_data(image_paths, labels, batch_size=1000, output_dir="data")
    
    # Visualize some images (optional)
    with np.load("data/batch_0.npz") as data:
        visualize_images(data["images"], data["labels"], num_samples=5)

    # Split and save the final data
    split_and_save_data(output_dir="data")

# Run the script
if __name__ == "__main__":
    main()